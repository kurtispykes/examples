{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G-p9E6ZHFbWE",
        "outputId": "f7e1c969-b5e9-4c8e-b467-b950ba4f5952"
      },
      "source": [
        "pip install layer-sdk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting layer-sdk\n",
            "  Downloading layer_sdk-0.7.3-py3-none-any.whl (26.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6 MB 77 kB/s \n",
            "\u001b[?25hCollecting GitPython==3.1.14\n",
            "  Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting click>=8.0\n",
            "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (2.6.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting aiodocker>=0.19.1\n",
            "  Downloading aiodocker-0.21.0-py3-none-any.whl (34 kB)\n",
            "Collecting grpcio-tools>=1.39.0\n",
            "  Downloading grpcio_tools-1.39.0-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting aiohttp[speedups]>=3.7.3\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 60.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting validators==0.18.2\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (3.17.3)\n",
            "Collecting yarl>=1.6.3\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.24\n",
            "  Downloading boto3-1.18.29-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.11.3 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (2.11.3)\n",
            "Collecting mlflow>=1.15.0\n",
            "  Downloading mlflow-1.20.1-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 88 kB/s \n",
            "\u001b[?25hCollecting rich>=9.10.0\n",
            "  Downloading rich-10.7.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.39.0 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (1.39.0)\n",
            "Collecting polling>=0.3.1\n",
            "  Downloading polling-0.3.2.tar.gz (5.2 kB)\n",
            "Requirement already satisfied: pandas>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (1.1.5)\n",
            "Collecting pyarrow>=4.0.0\n",
            "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.6 MB 96 kB/s \n",
            "\u001b[?25hCollecting pyspark>=3.0.1\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 71 kB/s \n",
            "\u001b[?25hCollecting prompt-toolkit>=3.0.8\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 73.2 MB/s \n",
            "\u001b[?25hCollecting pyjwt<2.0.0,>=1.7.1\n",
            "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting validate-email==1.3\n",
            "  Downloading validate_email-1.3.tar.gz (4.7 kB)\n",
            "Collecting cryptography>=3.4.7\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3 in /usr/local/lib/python3.7/dist-packages (from layer-sdk) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators==0.18.2->layer-sdk) (4.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators==0.18.2->layer-sdk) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiodocker>=0.19.1->layer-sdk) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp[speedups]>=3.7.3->layer-sdk) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp[speedups]>=3.7.3->layer-sdk) (21.2.0)\n",
            "Collecting brotlipy\n",
            "  Downloading brotlipy-0.7.0.tar.gz (413 kB)\n",
            "\u001b[K     |████████████████████████████████| 413 kB 79.7 MB/s \n",
            "\u001b[?25hCollecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.29\n",
            "  Downloading botocore-1.21.29-py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 49.7 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.29->boto3>=1.16.24->layer-sdk) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0->layer-sdk) (4.6.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.7->layer-sdk) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.7->layer-sdk) (2.20)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from grpcio-tools>=1.39.0->layer-sdk) (57.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.11.3->layer-sdk) (2.0.1)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (1.3.0)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (2018.9)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (1.4.22)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (21.0)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (0.4.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (0.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (1.1.4)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow>=1.15.0->layer-sdk) (2.23.0)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow>=1.15.0->layer-sdk) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0->layer-sdk) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit>=3.0.8->layer-sdk) (0.2.5)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 75.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow>=1.15.0->layer-sdk) (2021.5.30)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=9.10.0->layer-sdk) (2.6.1)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow>=1.15.0->layer-sdk) (1.1.1)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.0.0-cp37-cp37m-manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow>=1.15.0->layer-sdk) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow>=1.15.0->layer-sdk) (1.1.0)\n",
            "Collecting Flask\n",
            "  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting Werkzeug>=2.0\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
            "Collecting Jinja2>=2.11.3\n",
            "  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow>=1.15.0->layer-sdk) (2.4.7)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow>=1.15.0->layer-sdk) (0.11.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->layer-sdk) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->layer-sdk) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->layer-sdk) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->layer-sdk) (1.0.1)\n",
            "Building wheels for collected packages: validate-email, alembic, databricks-cli, polling, pyspark, brotlipy, prometheus-flask-exporter\n",
            "  Building wheel for validate-email (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validate-email: filename=validate_email-1.3-py3-none-any.whl size=5482 sha256=c9e7c976a76c4ac300e0d236c344798f6e620c537dca5d152a23da5916ac333e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/8f/92/c43287715852eaa75e0d8aa1941c481072b4a82c4f4975074e\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=1f9c0aef453af4ebe4c31b6e1765decd0b8208b574860c53068c3673615bfef9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105260 sha256=b67e20e80c68898003ef0c2419795695d6774f204badc1fce080689707b1eab9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n",
            "  Building wheel for polling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polling: filename=polling-0.3.2-py3-none-any.whl size=4129 sha256=0cd197d8e55c558d01499bff2a728640e0f5ef5c8bcb66f2b62942ae9b22d0ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/3f/0c/54a03b715fce3176335c957ae94d7d0b2a918e89b1b195bace\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=c00be7e2c6cf0bec42a88181adf103a61ca2db0a47ba4f2513d07734b6dd5ecd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "  Building wheel for brotlipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for brotlipy: filename=brotlipy-0.7.0-cp37-cp37m-linux_x86_64.whl size=830296 sha256=9c3ebd21f98fdbe908e70e8c6a1fba33f3ec8ce737072113a5b5b01a753100f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d4/c1/a9feab7aed23e7a3d0519214b48686c2936c327bbb743b575e\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17415 sha256=9e56214dea62872d4c39ce70298564fa717d92e4a71777a88dead1e2bb267f80\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n",
            "Successfully built validate-email alembic databricks-cli polling pyspark brotlipy prometheus-flask-exporter\n",
            "Installing collected packages: Werkzeug, urllib3, smmap, multidict, jmespath, Jinja2, itsdangerous, click, yarl, websocket-client, python-editor, pycares, Mako, gitdb, Flask, botocore, async-timeout, tokenizers, sacremoses, s3transfer, querystring-parser, pyyaml, py4j, prometheus-flask-exporter, huggingface-hub, gunicorn, GitPython, docker, databricks-cli, commonmark, colorama, cchardet, brotlipy, alembic, aiohttp, aiodns, validators, validate-email, transformers, rich, pyspark, pyjwt, pyarrow, prompt-toolkit, polling, mlflow, grpcio-tools, cryptography, boto3, aiodocker, layer-sdk\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 1.1.0\n",
            "    Uninstalling itsdangerous-1.1.0:\n",
            "      Successfully uninstalled itsdangerous-1.1.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "ipython 5.5.0 requires prompt-toolkit<2.0.0,>=1.0.4, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Flask-2.0.1 GitPython-3.1.14 Jinja2-3.0.1 Mako-1.1.5 Werkzeug-2.0.1 aiodns-3.0.0 aiodocker-0.21.0 aiohttp-3.7.4.post0 alembic-1.4.1 async-timeout-3.0.1 boto3-1.18.29 botocore-1.21.29 brotlipy-0.7.0 cchardet-2.1.7 click-8.0.1 colorama-0.4.4 commonmark-0.9.1 cryptography-3.4.8 databricks-cli-0.15.0 docker-5.0.0 gitdb-4.0.7 grpcio-tools-1.39.0 gunicorn-20.1.0 huggingface-hub-0.0.12 itsdangerous-2.0.1 jmespath-0.10.0 layer-sdk-0.7.3 mlflow-1.20.1 multidict-5.1.0 polling-0.3.2 prometheus-flask-exporter-0.18.2 prompt-toolkit-3.0.20 py4j-0.10.9 pyarrow-5.0.0 pycares-4.0.0 pyjwt-1.7.1 pyspark-3.1.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 rich-10.7.0 s3transfer-0.5.0 sacremoses-0.0.45 smmap-4.0.0 tokenizers-0.10.3 transformers-4.9.2 urllib3-1.25.11 validate-email-1.3 validators-0.18.2 websocket-client-1.2.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "iF2TPf9vF8ds",
        "outputId": "b47225bb-887a-45e2-a1bc-dcac182d9905"
      },
      "source": [
        "pip install -U ipython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Collecting ipython\n",
            "  Downloading ipython-7.26.0-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython) (0.1.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.0.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython) (0.18.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (3.0.20)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
            "Installing collected packages: ipython\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.26.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.26.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThsuXw9SFeW6"
      },
      "source": [
        "import layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8oWoy9wFhPM",
        "outputId": "7fbd5869-5afb-4dee-82b2-4c2fa26d300b"
      },
      "source": [
        "layer.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please open the following link in your web browser. Once logged in, copy the code and paste it here.\n",
            "https://auth.beta.layer.co/authorize?response_type=code&code_challenge=nPHRp-C1JplMLZlgLu_AIlilP0lEzVk2T_HoDJiMIec&code_challenge_method=S256&client_id=tvp1Wo8acTt4z46c7MbUB3Rg5MqfmqkN&redirect_uri=https://beta.layer.co/oauth/code&scope=offline_access&audience=https://beta.layer.co\n",
            "Code: -y4X5JT2EZwPRNTC\n",
            "Successfully logged into https://beta.layer.co\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GarEoyX3tgx3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import wget\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Dense,GlobalAveragePooling1D,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import keras"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUZC8iWRFhFy"
      },
      "source": [
        "df = layer.get_dataset(\"spam_messages\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-OcsoEdFhDH"
      },
      "source": [
        "df = df.to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CXd9dxXDFqjF",
        "outputId": "96bf7d0d-eb0a-4d34-e65f-e95951cf7f0a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>punct</th>\n",
              "      <th>message</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  id  punct                                            message  length\n",
              "0   ham   0      9  Go until jurong point, crazy.. Available only ...     111\n",
              "1   ham   1      6                      Ok lar... Joking wif u oni...      29\n",
              "2  spam   2      6  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
              "3   ham   3      6  U dun say so early hor... U c already then say...      49\n",
              "4   ham   4      2  Nah I don't think he goes to usf, he lives aro...      61"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PZh1zu1Ftx_"
      },
      "source": [
        "from typing import Any\n",
        "from layer import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "df = df.assign(is_spam = labelencoder.fit_transform(df[\"label\"]))\n",
        "df.drop(columns=[\"label\"], inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "u-7MkP50GCR5",
        "outputId": "e79a8669-ea7c-4384-9969-52462c6a10f2"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>punct</th>\n",
              "      <th>message</th>\n",
              "      <th>length</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>5567</td>\n",
              "      <td>8</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>5568</td>\n",
              "      <td>1</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>5569</td>\n",
              "      <td>7</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>5570</td>\n",
              "      <td>1</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>125</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>5571</td>\n",
              "      <td>1</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  punct  ... length  is_spam\n",
              "5567  5567      8  ...    160        1\n",
              "5568  5568      1  ...     36        0\n",
              "5569  5569      7  ...     57        0\n",
              "5570  5570      1  ...    125        0\n",
              "5571  5571      1  ...     26        0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lGOr_0VGlPa",
        "outputId": "683e3605-d7b0-448c-e937-971fd4889e2e"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Tb3Q4NGmuG"
      },
      "source": [
        "X = df['message']\n",
        "y = df['is_spam']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LG9oubPGqym"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.15,\n",
        "        random_state=0\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzquoXz5i_n8",
        "outputId": "5d14f061-dc7f-4e6c-abce-9761cbd8e272"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4456    Storming msg: Wen u lift d phne, u say \"HELLO\"...\n",
              "690     <Forwarded from 448712404000>Please CALL 08712...\n",
              "944     And also I've sorta blown him off a couple tim...\n",
              "3768                  Sir Goodmorning, Once free call me.\n",
              "1189    All will come alive.better correct any good lo...\n",
              "Name: message, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meQKBT2GGt6P"
      },
      "source": [
        "vocab_size = 1000\n",
        "oov_token = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrA3W8sRGy9G"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5_vPQeNG3wh"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zey_A6CLG903"
      },
      "source": [
        "# word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BvoDKH3G_Mz"
      },
      "source": [
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "filename = wget.download(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLTU2IR9M8UL"
      },
      "source": [
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "  zip_ref.extractall('glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IXlGqCvvJ8l"
      },
      "source": [
        "max_length = 100\n",
        "padding_type = \"post\"\n",
        "truncation_type = \"post\"\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=padding_type,\n",
        "                                   truncating=truncation_type)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length,\n",
        "                                  padding=padding_type, truncating=truncation_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNkMouufM_-s"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, max_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgpEqKHPNHTT",
        "outputId": "997bd588-33fa-4f24-c910-475ecb000727"
      },
      "source": [
        "embeddings_index['forever']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02962  ,  0.02501  ,  0.55628  , -0.40107  ,  0.31203  ,\n",
              "        0.72898  , -0.16204  ,  0.0308   ,  0.35242  , -0.60221  ,\n",
              "       -0.030428 ,  0.86107  ,  0.70624  , -0.14869  , -0.28837  ,\n",
              "       -0.6763   ,  0.8031   ,  0.73101  , -0.080868 ,  0.35766  ,\n",
              "        0.22465  ,  0.36487  ,  0.024097 , -0.0072124,  0.45343  ,\n",
              "        0.85817  , -0.21828  , -0.59258  ,  1.0031   ,  0.0063343,\n",
              "        0.51643  ,  0.24284  , -0.13779  ,  0.046686 , -0.30697  ,\n",
              "       -0.037773 , -0.23124  , -0.73011  , -0.22249  , -1.1748   ,\n",
              "        0.302    ,  0.085231 ,  0.17523  ,  0.028652 , -0.31338  ,\n",
              "       -0.23467  ,  0.40314  ,  0.92388  , -0.24224  , -1.385    ,\n",
              "        0.015852 , -0.49981  ,  0.13854  ,  1.1617   ,  0.35233  ,\n",
              "       -1.5063   ,  0.28455  ,  0.67725  ,  0.44386  ,  0.21196  ,\n",
              "       -0.13162  ,  1.1688   , -0.74866  , -0.82651  ,  0.7592   ,\n",
              "        0.49249  ,  0.78703  ,  0.43928  , -0.063966 , -0.55001  ,\n",
              "        0.4577   , -0.1529   ,  0.058072 ,  0.14943  ,  0.10822  ,\n",
              "       -0.25844  ,  0.39644  ,  0.050967 , -0.36245  , -0.094291 ,\n",
              "       -0.54595  , -0.059317 ,  0.22066  ,  0.72716  , -0.36983  ,\n",
              "        0.711    , -0.52005  , -0.37295  , -0.37943  ,  0.40011  ,\n",
              "       -0.67684  ,  0.16771  ,  0.53512  , -0.076841 , -0.59374  ,\n",
              "       -0.13164  ,  0.15354  , -0.0043013, -0.23466  ,  0.084479 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy4Mka4xNryB"
      },
      "source": [
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHU5ASuuOXbm"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                max_length,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_length,\n",
        "                                trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFZM9OC9OyzL",
        "outputId": "566d4881-d679-4bc8-8c5f-d100ad429f93"
      },
      "source": [
        "len(word_index) + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8327"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALyQfh42PQT4"
      },
      "source": [
        "# word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec-tvMiGPUiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4aad57-3d3a-4c28-e792-c18894a9f576"
      },
      "source": [
        "word_embedding_model = Sequential([\n",
        "        embedding_layer,\n",
        "        Dropout(0.2),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "word_embedding_model.compile(optimizer='adam',\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "callbacks = [\n",
        "        EarlyStopping(patience=2),\n",
        "    ]\n",
        "num_epochs = 600\n",
        "word_embedding_model.fit(X_train_padded, y_train, epochs=num_epochs, validation_data=(X_test_padded, y_test),\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "loss, accuracy = word_embedding_model.evaluate(X_train_padded, y_train, verbose=1)\n",
        "\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4994: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "148/148 [==============================] - 1s 6ms/step - loss: 0.6074 - binary_accuracy: 0.8541 - val_loss: 0.5489 - val_binary_accuracy: 0.8636\n",
            "Epoch 2/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.5202 - binary_accuracy: 0.8663 - val_loss: 0.4936 - val_binary_accuracy: 0.8636\n",
            "Epoch 3/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.4813 - binary_accuracy: 0.8663 - val_loss: 0.4647 - val_binary_accuracy: 0.8636\n",
            "Epoch 4/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.4565 - binary_accuracy: 0.8663 - val_loss: 0.4449 - val_binary_accuracy: 0.8636\n",
            "Epoch 5/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.4393 - binary_accuracy: 0.8663 - val_loss: 0.4291 - val_binary_accuracy: 0.8636\n",
            "Epoch 6/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.4245 - binary_accuracy: 0.8663 - val_loss: 0.4148 - val_binary_accuracy: 0.8636\n",
            "Epoch 7/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.4104 - binary_accuracy: 0.8663 - val_loss: 0.4013 - val_binary_accuracy: 0.8636\n",
            "Epoch 8/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3984 - binary_accuracy: 0.8663 - val_loss: 0.3881 - val_binary_accuracy: 0.8636\n",
            "Epoch 9/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3852 - binary_accuracy: 0.8663 - val_loss: 0.3756 - val_binary_accuracy: 0.8636\n",
            "Epoch 10/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3731 - binary_accuracy: 0.8663 - val_loss: 0.3632 - val_binary_accuracy: 0.8636\n",
            "Epoch 11/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3609 - binary_accuracy: 0.8663 - val_loss: 0.3517 - val_binary_accuracy: 0.8636\n",
            "Epoch 12/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3493 - binary_accuracy: 0.8666 - val_loss: 0.3405 - val_binary_accuracy: 0.8636\n",
            "Epoch 13/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3388 - binary_accuracy: 0.8674 - val_loss: 0.3299 - val_binary_accuracy: 0.8636\n",
            "Epoch 14/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3281 - binary_accuracy: 0.8687 - val_loss: 0.3201 - val_binary_accuracy: 0.8648\n",
            "Epoch 15/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3192 - binary_accuracy: 0.8697 - val_loss: 0.3107 - val_binary_accuracy: 0.8684\n",
            "Epoch 16/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3100 - binary_accuracy: 0.8712 - val_loss: 0.3018 - val_binary_accuracy: 0.8696\n",
            "Epoch 17/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.3011 - binary_accuracy: 0.8758 - val_loss: 0.2934 - val_binary_accuracy: 0.8744\n",
            "Epoch 18/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2963 - binary_accuracy: 0.8786 - val_loss: 0.2857 - val_binary_accuracy: 0.8756\n",
            "Epoch 19/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2889 - binary_accuracy: 0.8801 - val_loss: 0.2784 - val_binary_accuracy: 0.8804\n",
            "Epoch 20/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2802 - binary_accuracy: 0.8847 - val_loss: 0.2716 - val_binary_accuracy: 0.8852\n",
            "Epoch 21/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2742 - binary_accuracy: 0.8875 - val_loss: 0.2651 - val_binary_accuracy: 0.8900\n",
            "Epoch 22/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2685 - binary_accuracy: 0.8881 - val_loss: 0.2590 - val_binary_accuracy: 0.8947\n",
            "Epoch 23/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2614 - binary_accuracy: 0.8938 - val_loss: 0.2532 - val_binary_accuracy: 0.8971\n",
            "Epoch 24/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2578 - binary_accuracy: 0.8982 - val_loss: 0.2478 - val_binary_accuracy: 0.9055\n",
            "Epoch 25/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2520 - binary_accuracy: 0.9010 - val_loss: 0.2426 - val_binary_accuracy: 0.9103\n",
            "Epoch 26/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2469 - binary_accuracy: 0.8995 - val_loss: 0.2378 - val_binary_accuracy: 0.9127\n",
            "Epoch 27/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2415 - binary_accuracy: 0.9037 - val_loss: 0.2332 - val_binary_accuracy: 0.9151\n",
            "Epoch 28/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2416 - binary_accuracy: 0.9058 - val_loss: 0.2291 - val_binary_accuracy: 0.9175\n",
            "Epoch 29/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2379 - binary_accuracy: 0.9048 - val_loss: 0.2250 - val_binary_accuracy: 0.9199\n",
            "Epoch 30/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2300 - binary_accuracy: 0.9139 - val_loss: 0.2211 - val_binary_accuracy: 0.9199\n",
            "Epoch 31/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2280 - binary_accuracy: 0.9139 - val_loss: 0.2176 - val_binary_accuracy: 0.9199\n",
            "Epoch 32/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2257 - binary_accuracy: 0.9113 - val_loss: 0.2142 - val_binary_accuracy: 0.9199\n",
            "Epoch 33/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2212 - binary_accuracy: 0.9111 - val_loss: 0.2108 - val_binary_accuracy: 0.9234\n",
            "Epoch 34/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2181 - binary_accuracy: 0.9168 - val_loss: 0.2077 - val_binary_accuracy: 0.9246\n",
            "Epoch 35/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2152 - binary_accuracy: 0.9177 - val_loss: 0.2048 - val_binary_accuracy: 0.9246\n",
            "Epoch 36/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2137 - binary_accuracy: 0.9164 - val_loss: 0.2020 - val_binary_accuracy: 0.9282\n",
            "Epoch 37/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2114 - binary_accuracy: 0.9191 - val_loss: 0.1993 - val_binary_accuracy: 0.9282\n",
            "Epoch 38/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2076 - binary_accuracy: 0.9200 - val_loss: 0.1967 - val_binary_accuracy: 0.9282\n",
            "Epoch 39/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2048 - binary_accuracy: 0.9240 - val_loss: 0.1943 - val_binary_accuracy: 0.9294\n",
            "Epoch 40/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2045 - binary_accuracy: 0.9231 - val_loss: 0.1920 - val_binary_accuracy: 0.9294\n",
            "Epoch 41/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2002 - binary_accuracy: 0.9261 - val_loss: 0.1897 - val_binary_accuracy: 0.9306\n",
            "Epoch 42/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.2001 - binary_accuracy: 0.9263 - val_loss: 0.1875 - val_binary_accuracy: 0.9306\n",
            "Epoch 43/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1997 - binary_accuracy: 0.9280 - val_loss: 0.1855 - val_binary_accuracy: 0.9318\n",
            "Epoch 44/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1963 - binary_accuracy: 0.9255 - val_loss: 0.1836 - val_binary_accuracy: 0.9330\n",
            "Epoch 45/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1928 - binary_accuracy: 0.9303 - val_loss: 0.1816 - val_binary_accuracy: 0.9330\n",
            "Epoch 46/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1898 - binary_accuracy: 0.9316 - val_loss: 0.1798 - val_binary_accuracy: 0.9354\n",
            "Epoch 47/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1955 - binary_accuracy: 0.9244 - val_loss: 0.1782 - val_binary_accuracy: 0.9354\n",
            "Epoch 48/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1882 - binary_accuracy: 0.9295 - val_loss: 0.1764 - val_binary_accuracy: 0.9354\n",
            "Epoch 49/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1855 - binary_accuracy: 0.9339 - val_loss: 0.1747 - val_binary_accuracy: 0.9354\n",
            "Epoch 50/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1898 - binary_accuracy: 0.9295 - val_loss: 0.1733 - val_binary_accuracy: 0.9354\n",
            "Epoch 51/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1880 - binary_accuracy: 0.9305 - val_loss: 0.1719 - val_binary_accuracy: 0.9366\n",
            "Epoch 52/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1824 - binary_accuracy: 0.9343 - val_loss: 0.1704 - val_binary_accuracy: 0.9366\n",
            "Epoch 53/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1817 - binary_accuracy: 0.9350 - val_loss: 0.1690 - val_binary_accuracy: 0.9366\n",
            "Epoch 54/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1809 - binary_accuracy: 0.9356 - val_loss: 0.1677 - val_binary_accuracy: 0.9366\n",
            "Epoch 55/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1784 - binary_accuracy: 0.9326 - val_loss: 0.1663 - val_binary_accuracy: 0.9366\n",
            "Epoch 56/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1771 - binary_accuracy: 0.9350 - val_loss: 0.1650 - val_binary_accuracy: 0.9390\n",
            "Epoch 57/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1763 - binary_accuracy: 0.9369 - val_loss: 0.1639 - val_binary_accuracy: 0.9378\n",
            "Epoch 58/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1764 - binary_accuracy: 0.9377 - val_loss: 0.1626 - val_binary_accuracy: 0.9390\n",
            "Epoch 59/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1772 - binary_accuracy: 0.9341 - val_loss: 0.1616 - val_binary_accuracy: 0.9390\n",
            "Epoch 60/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1747 - binary_accuracy: 0.9381 - val_loss: 0.1606 - val_binary_accuracy: 0.9390\n",
            "Epoch 61/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1710 - binary_accuracy: 0.9390 - val_loss: 0.1593 - val_binary_accuracy: 0.9390\n",
            "Epoch 62/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1707 - binary_accuracy: 0.9379 - val_loss: 0.1583 - val_binary_accuracy: 0.9390\n",
            "Epoch 63/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1699 - binary_accuracy: 0.9411 - val_loss: 0.1573 - val_binary_accuracy: 0.9390\n",
            "Epoch 64/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1727 - binary_accuracy: 0.9371 - val_loss: 0.1563 - val_binary_accuracy: 0.9390\n",
            "Epoch 65/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1671 - binary_accuracy: 0.9426 - val_loss: 0.1552 - val_binary_accuracy: 0.9402\n",
            "Epoch 66/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1686 - binary_accuracy: 0.9394 - val_loss: 0.1543 - val_binary_accuracy: 0.9402\n",
            "Epoch 67/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1657 - binary_accuracy: 0.9436 - val_loss: 0.1534 - val_binary_accuracy: 0.9402\n",
            "Epoch 68/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1661 - binary_accuracy: 0.9405 - val_loss: 0.1524 - val_binary_accuracy: 0.9414\n",
            "Epoch 69/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1668 - binary_accuracy: 0.9413 - val_loss: 0.1516 - val_binary_accuracy: 0.9414\n",
            "Epoch 70/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1635 - binary_accuracy: 0.9421 - val_loss: 0.1508 - val_binary_accuracy: 0.9414\n",
            "Epoch 71/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1640 - binary_accuracy: 0.9424 - val_loss: 0.1500 - val_binary_accuracy: 0.9414\n",
            "Epoch 72/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1635 - binary_accuracy: 0.9413 - val_loss: 0.1492 - val_binary_accuracy: 0.9414\n",
            "Epoch 73/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1592 - binary_accuracy: 0.9430 - val_loss: 0.1483 - val_binary_accuracy: 0.9414\n",
            "Epoch 74/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1624 - binary_accuracy: 0.9447 - val_loss: 0.1475 - val_binary_accuracy: 0.9426\n",
            "Epoch 75/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1594 - binary_accuracy: 0.9436 - val_loss: 0.1468 - val_binary_accuracy: 0.9426\n",
            "Epoch 76/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1620 - binary_accuracy: 0.9438 - val_loss: 0.1461 - val_binary_accuracy: 0.9426\n",
            "Epoch 77/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1580 - binary_accuracy: 0.9449 - val_loss: 0.1454 - val_binary_accuracy: 0.9426\n",
            "Epoch 78/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1589 - binary_accuracy: 0.9440 - val_loss: 0.1447 - val_binary_accuracy: 0.9426\n",
            "Epoch 79/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1555 - binary_accuracy: 0.9462 - val_loss: 0.1440 - val_binary_accuracy: 0.9414\n",
            "Epoch 80/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1565 - binary_accuracy: 0.9434 - val_loss: 0.1435 - val_binary_accuracy: 0.9414\n",
            "Epoch 81/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1604 - binary_accuracy: 0.9411 - val_loss: 0.1429 - val_binary_accuracy: 0.9414\n",
            "Epoch 82/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1572 - binary_accuracy: 0.9440 - val_loss: 0.1423 - val_binary_accuracy: 0.9414\n",
            "Epoch 83/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1523 - binary_accuracy: 0.9468 - val_loss: 0.1416 - val_binary_accuracy: 0.9414\n",
            "Epoch 84/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1545 - binary_accuracy: 0.9447 - val_loss: 0.1410 - val_binary_accuracy: 0.9414\n",
            "Epoch 85/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1568 - binary_accuracy: 0.9426 - val_loss: 0.1404 - val_binary_accuracy: 0.9414\n",
            "Epoch 86/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1570 - binary_accuracy: 0.9453 - val_loss: 0.1399 - val_binary_accuracy: 0.9414\n",
            "Epoch 87/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1534 - binary_accuracy: 0.9449 - val_loss: 0.1394 - val_binary_accuracy: 0.9414\n",
            "Epoch 88/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1506 - binary_accuracy: 0.9470 - val_loss: 0.1389 - val_binary_accuracy: 0.9414\n",
            "Epoch 89/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1512 - binary_accuracy: 0.9451 - val_loss: 0.1383 - val_binary_accuracy: 0.9402\n",
            "Epoch 90/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1481 - binary_accuracy: 0.9470 - val_loss: 0.1377 - val_binary_accuracy: 0.9390\n",
            "Epoch 91/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1484 - binary_accuracy: 0.9489 - val_loss: 0.1372 - val_binary_accuracy: 0.9390\n",
            "Epoch 92/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1501 - binary_accuracy: 0.9466 - val_loss: 0.1367 - val_binary_accuracy: 0.9390\n",
            "Epoch 93/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1512 - binary_accuracy: 0.9483 - val_loss: 0.1363 - val_binary_accuracy: 0.9390\n",
            "Epoch 94/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1503 - binary_accuracy: 0.9483 - val_loss: 0.1358 - val_binary_accuracy: 0.9390\n",
            "Epoch 95/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1454 - binary_accuracy: 0.9502 - val_loss: 0.1353 - val_binary_accuracy: 0.9390\n",
            "Epoch 96/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1466 - binary_accuracy: 0.9502 - val_loss: 0.1349 - val_binary_accuracy: 0.9390\n",
            "Epoch 97/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1495 - binary_accuracy: 0.9487 - val_loss: 0.1344 - val_binary_accuracy: 0.9390\n",
            "Epoch 98/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1465 - binary_accuracy: 0.9476 - val_loss: 0.1340 - val_binary_accuracy: 0.9390\n",
            "Epoch 99/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1499 - binary_accuracy: 0.9468 - val_loss: 0.1335 - val_binary_accuracy: 0.9390\n",
            "Epoch 100/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1464 - binary_accuracy: 0.9504 - val_loss: 0.1330 - val_binary_accuracy: 0.9390\n",
            "Epoch 101/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1486 - binary_accuracy: 0.9459 - val_loss: 0.1326 - val_binary_accuracy: 0.9390\n",
            "Epoch 102/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1497 - binary_accuracy: 0.9470 - val_loss: 0.1322 - val_binary_accuracy: 0.9390\n",
            "Epoch 103/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1446 - binary_accuracy: 0.9502 - val_loss: 0.1318 - val_binary_accuracy: 0.9390\n",
            "Epoch 104/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1418 - binary_accuracy: 0.9500 - val_loss: 0.1313 - val_binary_accuracy: 0.9390\n",
            "Epoch 105/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1450 - binary_accuracy: 0.9493 - val_loss: 0.1309 - val_binary_accuracy: 0.9390\n",
            "Epoch 106/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1431 - binary_accuracy: 0.9502 - val_loss: 0.1306 - val_binary_accuracy: 0.9390\n",
            "Epoch 107/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1434 - binary_accuracy: 0.9497 - val_loss: 0.1302 - val_binary_accuracy: 0.9390\n",
            "Epoch 108/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1431 - binary_accuracy: 0.9493 - val_loss: 0.1298 - val_binary_accuracy: 0.9390\n",
            "Epoch 109/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1463 - binary_accuracy: 0.9481 - val_loss: 0.1294 - val_binary_accuracy: 0.9402\n",
            "Epoch 110/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1397 - binary_accuracy: 0.9491 - val_loss: 0.1291 - val_binary_accuracy: 0.9390\n",
            "Epoch 111/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1436 - binary_accuracy: 0.9485 - val_loss: 0.1287 - val_binary_accuracy: 0.9402\n",
            "Epoch 112/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1388 - binary_accuracy: 0.9531 - val_loss: 0.1283 - val_binary_accuracy: 0.9402\n",
            "Epoch 113/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1422 - binary_accuracy: 0.9493 - val_loss: 0.1279 - val_binary_accuracy: 0.9426\n",
            "Epoch 114/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1405 - binary_accuracy: 0.9533 - val_loss: 0.1275 - val_binary_accuracy: 0.9426\n",
            "Epoch 115/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1451 - binary_accuracy: 0.9493 - val_loss: 0.1272 - val_binary_accuracy: 0.9426\n",
            "Epoch 116/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1403 - binary_accuracy: 0.9512 - val_loss: 0.1269 - val_binary_accuracy: 0.9426\n",
            "Epoch 117/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1395 - binary_accuracy: 0.9535 - val_loss: 0.1267 - val_binary_accuracy: 0.9426\n",
            "Epoch 118/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1437 - binary_accuracy: 0.9483 - val_loss: 0.1264 - val_binary_accuracy: 0.9426\n",
            "Epoch 119/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1444 - binary_accuracy: 0.9516 - val_loss: 0.1261 - val_binary_accuracy: 0.9426\n",
            "Epoch 120/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1403 - binary_accuracy: 0.9500 - val_loss: 0.1257 - val_binary_accuracy: 0.9426\n",
            "Epoch 121/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1408 - binary_accuracy: 0.9519 - val_loss: 0.1254 - val_binary_accuracy: 0.9426\n",
            "Epoch 122/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1423 - binary_accuracy: 0.9495 - val_loss: 0.1251 - val_binary_accuracy: 0.9426\n",
            "Epoch 123/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1383 - binary_accuracy: 0.9514 - val_loss: 0.1250 - val_binary_accuracy: 0.9426\n",
            "Epoch 124/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1394 - binary_accuracy: 0.9557 - val_loss: 0.1246 - val_binary_accuracy: 0.9426\n",
            "Epoch 125/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1384 - binary_accuracy: 0.9510 - val_loss: 0.1244 - val_binary_accuracy: 0.9426\n",
            "Epoch 126/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1379 - binary_accuracy: 0.9466 - val_loss: 0.1242 - val_binary_accuracy: 0.9426\n",
            "Epoch 127/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1404 - binary_accuracy: 0.9510 - val_loss: 0.1237 - val_binary_accuracy: 0.9462\n",
            "Epoch 128/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1382 - binary_accuracy: 0.9514 - val_loss: 0.1235 - val_binary_accuracy: 0.9462\n",
            "Epoch 129/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1343 - binary_accuracy: 0.9531 - val_loss: 0.1232 - val_binary_accuracy: 0.9462\n",
            "Epoch 130/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1367 - binary_accuracy: 0.9529 - val_loss: 0.1229 - val_binary_accuracy: 0.9462\n",
            "Epoch 131/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1393 - binary_accuracy: 0.9523 - val_loss: 0.1227 - val_binary_accuracy: 0.9438\n",
            "Epoch 132/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1374 - binary_accuracy: 0.9516 - val_loss: 0.1226 - val_binary_accuracy: 0.9426\n",
            "Epoch 133/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1400 - binary_accuracy: 0.9514 - val_loss: 0.1222 - val_binary_accuracy: 0.9462\n",
            "Epoch 134/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1377 - binary_accuracy: 0.9510 - val_loss: 0.1220 - val_binary_accuracy: 0.9462\n",
            "Epoch 135/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1359 - binary_accuracy: 0.9531 - val_loss: 0.1216 - val_binary_accuracy: 0.9474\n",
            "Epoch 136/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1366 - binary_accuracy: 0.9523 - val_loss: 0.1214 - val_binary_accuracy: 0.9474\n",
            "Epoch 137/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1352 - binary_accuracy: 0.9531 - val_loss: 0.1212 - val_binary_accuracy: 0.9462\n",
            "Epoch 138/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1349 - binary_accuracy: 0.9540 - val_loss: 0.1211 - val_binary_accuracy: 0.9462\n",
            "Epoch 139/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1351 - binary_accuracy: 0.9508 - val_loss: 0.1208 - val_binary_accuracy: 0.9474\n",
            "Epoch 140/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1396 - binary_accuracy: 0.9519 - val_loss: 0.1206 - val_binary_accuracy: 0.9474\n",
            "Epoch 141/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1310 - binary_accuracy: 0.9565 - val_loss: 0.1203 - val_binary_accuracy: 0.9498\n",
            "Epoch 142/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1357 - binary_accuracy: 0.9557 - val_loss: 0.1202 - val_binary_accuracy: 0.9474\n",
            "Epoch 143/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1354 - binary_accuracy: 0.9529 - val_loss: 0.1198 - val_binary_accuracy: 0.9522\n",
            "Epoch 144/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1387 - binary_accuracy: 0.9529 - val_loss: 0.1195 - val_binary_accuracy: 0.9522\n",
            "Epoch 145/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1379 - binary_accuracy: 0.9540 - val_loss: 0.1193 - val_binary_accuracy: 0.9533\n",
            "Epoch 146/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1348 - binary_accuracy: 0.9540 - val_loss: 0.1192 - val_binary_accuracy: 0.9498\n",
            "Epoch 147/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1332 - binary_accuracy: 0.9542 - val_loss: 0.1190 - val_binary_accuracy: 0.9522\n",
            "Epoch 148/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1317 - binary_accuracy: 0.9544 - val_loss: 0.1188 - val_binary_accuracy: 0.9522\n",
            "Epoch 149/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1306 - binary_accuracy: 0.9550 - val_loss: 0.1187 - val_binary_accuracy: 0.9510\n",
            "Epoch 150/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1315 - binary_accuracy: 0.9512 - val_loss: 0.1184 - val_binary_accuracy: 0.9533\n",
            "Epoch 151/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1344 - binary_accuracy: 0.9529 - val_loss: 0.1181 - val_binary_accuracy: 0.9545\n",
            "Epoch 152/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1333 - binary_accuracy: 0.9519 - val_loss: 0.1181 - val_binary_accuracy: 0.9510\n",
            "Epoch 153/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1318 - binary_accuracy: 0.9542 - val_loss: 0.1178 - val_binary_accuracy: 0.9533\n",
            "Epoch 154/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1340 - binary_accuracy: 0.9529 - val_loss: 0.1177 - val_binary_accuracy: 0.9510\n",
            "Epoch 155/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1317 - binary_accuracy: 0.9544 - val_loss: 0.1176 - val_binary_accuracy: 0.9510\n",
            "Epoch 156/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1317 - binary_accuracy: 0.9552 - val_loss: 0.1172 - val_binary_accuracy: 0.9545\n",
            "Epoch 157/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1289 - binary_accuracy: 0.9567 - val_loss: 0.1171 - val_binary_accuracy: 0.9545\n",
            "Epoch 158/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1336 - binary_accuracy: 0.9535 - val_loss: 0.1171 - val_binary_accuracy: 0.9510\n",
            "Epoch 159/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1365 - binary_accuracy: 0.9538 - val_loss: 0.1168 - val_binary_accuracy: 0.9545\n",
            "Epoch 160/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1300 - binary_accuracy: 0.9557 - val_loss: 0.1165 - val_binary_accuracy: 0.9545\n",
            "Epoch 161/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1306 - binary_accuracy: 0.9535 - val_loss: 0.1164 - val_binary_accuracy: 0.9545\n",
            "Epoch 162/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1326 - binary_accuracy: 0.9533 - val_loss: 0.1164 - val_binary_accuracy: 0.9510\n",
            "Epoch 163/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1280 - binary_accuracy: 0.9559 - val_loss: 0.1159 - val_binary_accuracy: 0.9545\n",
            "Epoch 164/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1305 - binary_accuracy: 0.9554 - val_loss: 0.1157 - val_binary_accuracy: 0.9545\n",
            "Epoch 165/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1318 - binary_accuracy: 0.9544 - val_loss: 0.1155 - val_binary_accuracy: 0.9545\n",
            "Epoch 166/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1272 - binary_accuracy: 0.9535 - val_loss: 0.1153 - val_binary_accuracy: 0.9545\n",
            "Epoch 167/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1251 - binary_accuracy: 0.9595 - val_loss: 0.1152 - val_binary_accuracy: 0.9545\n",
            "Epoch 168/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1295 - binary_accuracy: 0.9542 - val_loss: 0.1150 - val_binary_accuracy: 0.9545\n",
            "Epoch 169/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1364 - binary_accuracy: 0.9523 - val_loss: 0.1149 - val_binary_accuracy: 0.9545\n",
            "Epoch 170/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1280 - binary_accuracy: 0.9554 - val_loss: 0.1147 - val_binary_accuracy: 0.9545\n",
            "Epoch 171/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1260 - binary_accuracy: 0.9559 - val_loss: 0.1146 - val_binary_accuracy: 0.9545\n",
            "Epoch 172/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1311 - binary_accuracy: 0.9561 - val_loss: 0.1145 - val_binary_accuracy: 0.9545\n",
            "Epoch 173/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1295 - binary_accuracy: 0.9559 - val_loss: 0.1142 - val_binary_accuracy: 0.9557\n",
            "Epoch 174/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1268 - binary_accuracy: 0.9548 - val_loss: 0.1141 - val_binary_accuracy: 0.9557\n",
            "Epoch 175/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1309 - binary_accuracy: 0.9542 - val_loss: 0.1139 - val_binary_accuracy: 0.9557\n",
            "Epoch 176/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1260 - binary_accuracy: 0.9576 - val_loss: 0.1139 - val_binary_accuracy: 0.9545\n",
            "Epoch 177/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1273 - binary_accuracy: 0.9563 - val_loss: 0.1137 - val_binary_accuracy: 0.9557\n",
            "Epoch 178/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1326 - binary_accuracy: 0.9550 - val_loss: 0.1135 - val_binary_accuracy: 0.9557\n",
            "Epoch 179/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1296 - binary_accuracy: 0.9554 - val_loss: 0.1133 - val_binary_accuracy: 0.9557\n",
            "Epoch 180/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9561 - val_loss: 0.1131 - val_binary_accuracy: 0.9557\n",
            "Epoch 181/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1296 - binary_accuracy: 0.9561 - val_loss: 0.1129 - val_binary_accuracy: 0.9557\n",
            "Epoch 182/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1266 - binary_accuracy: 0.9544 - val_loss: 0.1128 - val_binary_accuracy: 0.9557\n",
            "Epoch 183/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9557 - val_loss: 0.1127 - val_binary_accuracy: 0.9557\n",
            "Epoch 184/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1276 - binary_accuracy: 0.9569 - val_loss: 0.1125 - val_binary_accuracy: 0.9557\n",
            "Epoch 185/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1281 - binary_accuracy: 0.9544 - val_loss: 0.1124 - val_binary_accuracy: 0.9557\n",
            "Epoch 186/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1265 - binary_accuracy: 0.9565 - val_loss: 0.1124 - val_binary_accuracy: 0.9557\n",
            "Epoch 187/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1221 - binary_accuracy: 0.9580 - val_loss: 0.1124 - val_binary_accuracy: 0.9557\n",
            "Epoch 188/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1327 - binary_accuracy: 0.9523 - val_loss: 0.1121 - val_binary_accuracy: 0.9557\n",
            "Epoch 189/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1303 - binary_accuracy: 0.9557 - val_loss: 0.1120 - val_binary_accuracy: 0.9557\n",
            "Epoch 190/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1270 - binary_accuracy: 0.9571 - val_loss: 0.1119 - val_binary_accuracy: 0.9557\n",
            "Epoch 191/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1248 - binary_accuracy: 0.9550 - val_loss: 0.1117 - val_binary_accuracy: 0.9557\n",
            "Epoch 192/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1296 - binary_accuracy: 0.9559 - val_loss: 0.1118 - val_binary_accuracy: 0.9557\n",
            "Epoch 193/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1266 - binary_accuracy: 0.9599 - val_loss: 0.1116 - val_binary_accuracy: 0.9557\n",
            "Epoch 194/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9582 - val_loss: 0.1112 - val_binary_accuracy: 0.9557\n",
            "Epoch 195/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1210 - binary_accuracy: 0.9576 - val_loss: 0.1112 - val_binary_accuracy: 0.9557\n",
            "Epoch 196/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1301 - binary_accuracy: 0.9540 - val_loss: 0.1110 - val_binary_accuracy: 0.9557\n",
            "Epoch 197/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1227 - binary_accuracy: 0.9586 - val_loss: 0.1109 - val_binary_accuracy: 0.9557\n",
            "Epoch 198/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1255 - binary_accuracy: 0.9571 - val_loss: 0.1108 - val_binary_accuracy: 0.9557\n",
            "Epoch 199/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1273 - binary_accuracy: 0.9561 - val_loss: 0.1108 - val_binary_accuracy: 0.9557\n",
            "Epoch 200/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1298 - binary_accuracy: 0.9544 - val_loss: 0.1106 - val_binary_accuracy: 0.9557\n",
            "Epoch 201/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1252 - binary_accuracy: 0.9576 - val_loss: 0.1106 - val_binary_accuracy: 0.9557\n",
            "Epoch 202/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1254 - binary_accuracy: 0.9573 - val_loss: 0.1104 - val_binary_accuracy: 0.9557\n",
            "Epoch 203/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1261 - binary_accuracy: 0.9580 - val_loss: 0.1103 - val_binary_accuracy: 0.9557\n",
            "Epoch 204/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1282 - binary_accuracy: 0.9569 - val_loss: 0.1101 - val_binary_accuracy: 0.9557\n",
            "Epoch 205/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1270 - binary_accuracy: 0.9565 - val_loss: 0.1101 - val_binary_accuracy: 0.9557\n",
            "Epoch 206/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1240 - binary_accuracy: 0.9565 - val_loss: 0.1099 - val_binary_accuracy: 0.9557\n",
            "Epoch 207/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1249 - binary_accuracy: 0.9542 - val_loss: 0.1098 - val_binary_accuracy: 0.9557\n",
            "Epoch 208/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1221 - binary_accuracy: 0.9582 - val_loss: 0.1097 - val_binary_accuracy: 0.9557\n",
            "Epoch 209/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1252 - binary_accuracy: 0.9563 - val_loss: 0.1095 - val_binary_accuracy: 0.9557\n",
            "Epoch 210/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1276 - binary_accuracy: 0.9563 - val_loss: 0.1095 - val_binary_accuracy: 0.9557\n",
            "Epoch 211/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1260 - binary_accuracy: 0.9565 - val_loss: 0.1095 - val_binary_accuracy: 0.9557\n",
            "Epoch 212/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1268 - binary_accuracy: 0.9569 - val_loss: 0.1093 - val_binary_accuracy: 0.9557\n",
            "Epoch 213/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1248 - binary_accuracy: 0.9571 - val_loss: 0.1092 - val_binary_accuracy: 0.9557\n",
            "Epoch 214/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1258 - binary_accuracy: 0.9559 - val_loss: 0.1092 - val_binary_accuracy: 0.9557\n",
            "Epoch 215/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1251 - binary_accuracy: 0.9565 - val_loss: 0.1090 - val_binary_accuracy: 0.9557\n",
            "Epoch 216/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1230 - binary_accuracy: 0.9592 - val_loss: 0.1089 - val_binary_accuracy: 0.9557\n",
            "Epoch 217/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9573 - val_loss: 0.1086 - val_binary_accuracy: 0.9545\n",
            "Epoch 218/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1249 - binary_accuracy: 0.9573 - val_loss: 0.1087 - val_binary_accuracy: 0.9557\n",
            "Epoch 219/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1265 - binary_accuracy: 0.9565 - val_loss: 0.1085 - val_binary_accuracy: 0.9557\n",
            "Epoch 220/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1271 - binary_accuracy: 0.9550 - val_loss: 0.1084 - val_binary_accuracy: 0.9557\n",
            "Epoch 221/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1221 - binary_accuracy: 0.9571 - val_loss: 0.1083 - val_binary_accuracy: 0.9557\n",
            "Epoch 222/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1206 - binary_accuracy: 0.9586 - val_loss: 0.1081 - val_binary_accuracy: 0.9545\n",
            "Epoch 223/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1305 - binary_accuracy: 0.9569 - val_loss: 0.1082 - val_binary_accuracy: 0.9557\n",
            "Epoch 224/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1229 - binary_accuracy: 0.9557 - val_loss: 0.1080 - val_binary_accuracy: 0.9557\n",
            "Epoch 225/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1215 - binary_accuracy: 0.9582 - val_loss: 0.1080 - val_binary_accuracy: 0.9557\n",
            "Epoch 226/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1281 - binary_accuracy: 0.9567 - val_loss: 0.1081 - val_binary_accuracy: 0.9557\n",
            "Epoch 227/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1246 - binary_accuracy: 0.9548 - val_loss: 0.1079 - val_binary_accuracy: 0.9557\n",
            "Epoch 228/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9573 - val_loss: 0.1077 - val_binary_accuracy: 0.9557\n",
            "Epoch 229/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1232 - binary_accuracy: 0.9580 - val_loss: 0.1076 - val_binary_accuracy: 0.9557\n",
            "Epoch 230/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1228 - binary_accuracy: 0.9571 - val_loss: 0.1076 - val_binary_accuracy: 0.9557\n",
            "Epoch 231/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1227 - binary_accuracy: 0.9571 - val_loss: 0.1075 - val_binary_accuracy: 0.9557\n",
            "Epoch 232/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1226 - binary_accuracy: 0.9571 - val_loss: 0.1074 - val_binary_accuracy: 0.9557\n",
            "Epoch 233/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1159 - binary_accuracy: 0.9599 - val_loss: 0.1073 - val_binary_accuracy: 0.9557\n",
            "Epoch 234/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1231 - binary_accuracy: 0.9563 - val_loss: 0.1072 - val_binary_accuracy: 0.9557\n",
            "Epoch 235/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1298 - binary_accuracy: 0.9550 - val_loss: 0.1073 - val_binary_accuracy: 0.9557\n",
            "Epoch 236/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1228 - binary_accuracy: 0.9580 - val_loss: 0.1071 - val_binary_accuracy: 0.9557\n",
            "Epoch 237/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1240 - binary_accuracy: 0.9578 - val_loss: 0.1070 - val_binary_accuracy: 0.9557\n",
            "Epoch 238/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1221 - binary_accuracy: 0.9580 - val_loss: 0.1069 - val_binary_accuracy: 0.9557\n",
            "Epoch 239/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1237 - binary_accuracy: 0.9588 - val_loss: 0.1068 - val_binary_accuracy: 0.9557\n",
            "Epoch 240/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1245 - binary_accuracy: 0.9563 - val_loss: 0.1068 - val_binary_accuracy: 0.9557\n",
            "Epoch 241/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1207 - binary_accuracy: 0.9599 - val_loss: 0.1068 - val_binary_accuracy: 0.9557\n",
            "Epoch 242/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1175 - binary_accuracy: 0.9614 - val_loss: 0.1066 - val_binary_accuracy: 0.9557\n",
            "Epoch 243/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1199 - binary_accuracy: 0.9571 - val_loss: 0.1065 - val_binary_accuracy: 0.9557\n",
            "Epoch 244/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1209 - binary_accuracy: 0.9592 - val_loss: 0.1065 - val_binary_accuracy: 0.9557\n",
            "Epoch 245/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1232 - binary_accuracy: 0.9599 - val_loss: 0.1064 - val_binary_accuracy: 0.9557\n",
            "Epoch 246/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1213 - binary_accuracy: 0.9569 - val_loss: 0.1062 - val_binary_accuracy: 0.9545\n",
            "Epoch 247/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1244 - binary_accuracy: 0.9592 - val_loss: 0.1060 - val_binary_accuracy: 0.9545\n",
            "Epoch 248/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1211 - binary_accuracy: 0.9580 - val_loss: 0.1060 - val_binary_accuracy: 0.9533\n",
            "Epoch 249/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1248 - binary_accuracy: 0.9546 - val_loss: 0.1060 - val_binary_accuracy: 0.9545\n",
            "Epoch 250/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1252 - binary_accuracy: 0.9592 - val_loss: 0.1058 - val_binary_accuracy: 0.9533\n",
            "Epoch 251/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1199 - binary_accuracy: 0.9586 - val_loss: 0.1058 - val_binary_accuracy: 0.9557\n",
            "Epoch 252/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1231 - binary_accuracy: 0.9582 - val_loss: 0.1058 - val_binary_accuracy: 0.9557\n",
            "Epoch 253/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1216 - binary_accuracy: 0.9590 - val_loss: 0.1058 - val_binary_accuracy: 0.9557\n",
            "Epoch 254/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1206 - binary_accuracy: 0.9567 - val_loss: 0.1057 - val_binary_accuracy: 0.9557\n",
            "Epoch 255/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1230 - binary_accuracy: 0.9578 - val_loss: 0.1056 - val_binary_accuracy: 0.9557\n",
            "Epoch 256/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1162 - binary_accuracy: 0.9614 - val_loss: 0.1054 - val_binary_accuracy: 0.9545\n",
            "Epoch 257/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1260 - binary_accuracy: 0.9588 - val_loss: 0.1053 - val_binary_accuracy: 0.9545\n",
            "Epoch 258/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1250 - binary_accuracy: 0.9567 - val_loss: 0.1052 - val_binary_accuracy: 0.9545\n",
            "Epoch 259/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1209 - binary_accuracy: 0.9605 - val_loss: 0.1052 - val_binary_accuracy: 0.9557\n",
            "Epoch 260/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1205 - binary_accuracy: 0.9582 - val_loss: 0.1052 - val_binary_accuracy: 0.9557\n",
            "Epoch 261/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1215 - binary_accuracy: 0.9588 - val_loss: 0.1050 - val_binary_accuracy: 0.9557\n",
            "Epoch 262/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1206 - binary_accuracy: 0.9590 - val_loss: 0.1050 - val_binary_accuracy: 0.9557\n",
            "Epoch 263/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1201 - binary_accuracy: 0.9584 - val_loss: 0.1049 - val_binary_accuracy: 0.9557\n",
            "Epoch 264/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1189 - binary_accuracy: 0.9576 - val_loss: 0.1048 - val_binary_accuracy: 0.9557\n",
            "Epoch 265/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1210 - binary_accuracy: 0.9584 - val_loss: 0.1048 - val_binary_accuracy: 0.9557\n",
            "Epoch 266/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1239 - binary_accuracy: 0.9552 - val_loss: 0.1048 - val_binary_accuracy: 0.9557\n",
            "Epoch 267/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1253 - binary_accuracy: 0.9563 - val_loss: 0.1047 - val_binary_accuracy: 0.9557\n",
            "Epoch 268/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1182 - binary_accuracy: 0.9626 - val_loss: 0.1047 - val_binary_accuracy: 0.9557\n",
            "Epoch 269/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1178 - binary_accuracy: 0.9588 - val_loss: 0.1044 - val_binary_accuracy: 0.9569\n",
            "Epoch 270/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1231 - binary_accuracy: 0.9592 - val_loss: 0.1043 - val_binary_accuracy: 0.9569\n",
            "Epoch 271/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1226 - binary_accuracy: 0.9588 - val_loss: 0.1044 - val_binary_accuracy: 0.9581\n",
            "Epoch 272/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1210 - binary_accuracy: 0.9563 - val_loss: 0.1042 - val_binary_accuracy: 0.9569\n",
            "Epoch 273/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1177 - binary_accuracy: 0.9605 - val_loss: 0.1041 - val_binary_accuracy: 0.9581\n",
            "Epoch 274/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1131 - binary_accuracy: 0.9609 - val_loss: 0.1040 - val_binary_accuracy: 0.9569\n",
            "Epoch 275/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1181 - binary_accuracy: 0.9601 - val_loss: 0.1042 - val_binary_accuracy: 0.9581\n",
            "Epoch 276/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1180 - binary_accuracy: 0.9578 - val_loss: 0.1039 - val_binary_accuracy: 0.9569\n",
            "Epoch 277/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1210 - binary_accuracy: 0.9567 - val_loss: 0.1040 - val_binary_accuracy: 0.9581\n",
            "Epoch 278/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1185 - binary_accuracy: 0.9603 - val_loss: 0.1038 - val_binary_accuracy: 0.9569\n",
            "Epoch 279/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1150 - binary_accuracy: 0.9599 - val_loss: 0.1038 - val_binary_accuracy: 0.9593\n",
            "Epoch 280/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1204 - binary_accuracy: 0.9609 - val_loss: 0.1038 - val_binary_accuracy: 0.9593\n",
            "Epoch 281/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1184 - binary_accuracy: 0.9595 - val_loss: 0.1037 - val_binary_accuracy: 0.9593\n",
            "Epoch 282/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1180 - binary_accuracy: 0.9601 - val_loss: 0.1035 - val_binary_accuracy: 0.9569\n",
            "Epoch 283/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1178 - binary_accuracy: 0.9573 - val_loss: 0.1035 - val_binary_accuracy: 0.9569\n",
            "Epoch 284/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1157 - binary_accuracy: 0.9637 - val_loss: 0.1034 - val_binary_accuracy: 0.9569\n",
            "Epoch 285/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1155 - binary_accuracy: 0.9595 - val_loss: 0.1033 - val_binary_accuracy: 0.9569\n",
            "Epoch 286/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1139 - binary_accuracy: 0.9609 - val_loss: 0.1032 - val_binary_accuracy: 0.9569\n",
            "Epoch 287/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1219 - binary_accuracy: 0.9573 - val_loss: 0.1032 - val_binary_accuracy: 0.9569\n",
            "Epoch 288/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1175 - binary_accuracy: 0.9584 - val_loss: 0.1031 - val_binary_accuracy: 0.9569\n",
            "Epoch 289/600\n",
            "148/148 [==============================] - 1s 5ms/step - loss: 0.1174 - binary_accuracy: 0.9614 - val_loss: 0.1031 - val_binary_accuracy: 0.9593\n",
            "Epoch 290/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1203 - binary_accuracy: 0.9597 - val_loss: 0.1033 - val_binary_accuracy: 0.9581\n",
            "Epoch 291/600\n",
            "148/148 [==============================] - 1s 6ms/step - loss: 0.1112 - binary_accuracy: 0.9637 - val_loss: 0.1032 - val_binary_accuracy: 0.9593\n",
            "148/148 [==============================] - 0s 2ms/step - loss: 0.1017 - binary_accuracy: 0.9626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSS0A3godvUQ",
        "outputId": "c10e5be5-5fd8-4cb3-d524-622af18b12de"
      },
      "source": [
        "X_test_padded[0]"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1, 130, 517,   7,   1, 142,   1,   7, 150, 268,  28,   7, 681,\n",
              "         1,  10, 142, 330,   1,  16, 268, 134, 142, 269,  16,   5, 367,\n",
              "       141,   8,   7, 681, 124,  10, 305, 367,   1, 268,  95,  10, 142,\n",
              "         1, 759,   1, 124,   1,   1,   1,  77,  29,   1, 142, 269,  16,\n",
              "         5, 318, 376,  26, 211,   1, 368, 559, 559, 142,  97,   2, 368,\n",
              "         1, 172,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-0spiEDbicw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be86cf7e-e0c1-450c-f7a0-19b0dc713693"
      },
      "source": [
        "prediction = word_embedding_model.predict(X_test_padded[513])"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RvJi9IWe2GE",
        "outputId": "171d445d-b015-4ac9-b047-fb8c177a085d"
      },
      "source": [
        "prediction[0]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uGFQcn4fELI",
        "outputId": "766ed7ae-7adb-46f3-d8fb-e5127a1e2e28"
      },
      "source": [
        "if prediction[0][0]>0.5:\n",
        "  print(\"Is spam\")\n",
        "else:\n",
        "   print(\"Is not Spam\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0sAFkWAG3HJ"
      },
      "source": [
        "# Using the Layer generated  model in a notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfnfwksNtb_6",
        "outputId": "d9d42a3b-5ff5-423f-ec94-cdd072b4645f"
      },
      "source": [
        "layer_model = layer.get_model(\"glove_word_embedding_model\")\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqHoO8LspZlY",
        "outputId": "5d7c7494-1cf7-45c7-f83a-735910b4e38b"
      },
      "source": [
        "print(layer_model.parameters)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NztWvb-zcJk",
        "outputId": "df869fd1-ae82-429e-e155-7e8cc8d88372"
      },
      "source": [
        "print(layer_model.metrics)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Training Loss': [(1629990168173, 0.09136352688074112)], 'Training Accuracy': [(1629990168163, 0.9679054021835327)], 'Testing Loss': [(1629990168266, 0.09355905652046204)], 'Testing Accuracy': [(1629990168253, 0.9617224931716919)]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8noSXLwaDlE",
        "outputId": "a53676b1-9eec-47ac-a4da-36549ae80bd9"
      },
      "source": [
        "trained_classifier = layer_model.get_train()\n",
        "trained_classifier"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f4001c04c90>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSDF_ewQmiNi",
        "outputId": "019c3f9e-790d-4e29-d3e3-ab1d0082eb88"
      },
      "source": [
        "np.array(y_test)[513]"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w33bqRJc0160"
      },
      "source": [
        "test_message =  np.expand_dims(X_test_padded[513], axis=0)\n",
        "\n",
        "second_prediction = trained_classifier.predict(test_message)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GzJm-v51-56",
        "outputId": "07d0d1a5-dc99-47e0-e65a-5770dc48719a"
      },
      "source": [
        "second_prediction"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92767197]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmYvv56MbIlA",
        "outputId": "d3352764-3c07-4a7c-c264-3f209a783465"
      },
      "source": [
        "if second_prediction[0][0]>0.5:\n",
        "  print(\"Is spam\")\n",
        "else:\n",
        "   print(\"Is not Spam\")"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ_hpOXPhaj9",
        "outputId": "651b0610-58ff-445d-e6af-e8f454d0a640"
      },
      "source": [
        "!curl --header \"Content-Type: application/json; format=pandas-records\" \\\n",
        "  --request POST \\\n",
        "  --data '[[197,  40,  22, 309,   3, 205,   7, 560, 293, 357,  21,   4,  18, 184,   5,   1, 145, 256,  17,   1,  49, 671, 325, 118,   1,   1,66,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0]]' \\\n",
        "https://46629a05-66e8-4180-8493-b650bb3acc8d.inferences.beta.layer.co/invocations"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{\"0\": 0.9276719093322754}]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSXiL1AzEr8i",
        "outputId": "a8f9015f-128a-45d7-a5fe-f7576e9d9fe9"
      },
      "source": [
        "np.array(y_test)[50]"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TWz2aznEt-o",
        "outputId": "d6bf0423-a181-47af-9c70-6a93d8d70b5d"
      },
      "source": [
        "X_test_padded[50]"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,  50,  22,   4, 168,  43,  10,  33,   1,   4,   5, 126,  64,\n",
              "         1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryQaZfu4Dqjb",
        "outputId": "e9fb6a40-3266-440d-836a-07c4a4732f4c"
      },
      "source": [
        "!curl --header \"Content-Type: application/json; format=pandas-records\" \\\n",
        "  --request POST \\\n",
        "  --data '[ [ 1,  50,  22,   4, 168,  43,  10,  33,   1,   4,   5, 126,  64,1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,0,   0,   0,   0,   0,   0,   0,   0,   0]]' \\\n",
        "https://46629a05-66e8-4180-8493-b650bb3acc8d.inferences.beta.layer.co/invocations"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{\"0\": 0.007864177227020264}]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7rTWAixDv9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}